<!DOCTYPE html>
<html lang="en">
<head>
<title>Yixiang Gao's Pilot of Project</title>
<meta charset="utf-8">
<link rel="stylesheet" type="text/css" href="MLP_math.css">
<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
</head>

<body>
<div id="wrapper">
    <header>
        <a href="Gao_pilot.html">
            <img src="PilotSite/website_logo.jpg" alt="Home" height="150">
        </a>
        <nav>
            <ul>
                <li><a href="Gao_pilot.html">Home</a></li>
                <li><a href="about_me.html">About Me</a></li>
                <li><a href="publications.html">Publications</a></li>
                <li class="cue"><a href="tutorials.html">Tutorials</a></li>
                <li><a href="projects.html">Projects</a></li>
            </ul>
        </nav>
    </header>

    <h1>A Mathematical Rundown for Multiple Layer Perceptron (MLP)</h1>

    <p>This is a quick study notes of showing the MLP algorithm in pure mathematical form. Often times, these information were abstracted by the softwares people are using. In this notes, it will show how to compute the outputs of each layer from all the neurons by hand and through linear algebra to understand the computation process for forward passes and backward passes (backpropagation).</p> 
    <p>To begin with, We first construct a simple MLP structure. Assume there are two inputs for each sample, one hidden layer with four neurons, and one output layer with only one neuron. No bias term just to keep it simple.</p>
    <p><img src="PilotSite/multilayer_perceptron.svg"></p>
    <p>To be continued...</p>

    <footer>
        &copy; Yixiang Gao &nbsp;Last Updated: 02/25/2019 &nbsp;<a href="mailto:yg5d6@mail.missouri.edu"><i class="material-icons" style="color: #000000">email</i></a> 
    </footer>

</div>
</body>
</html>
